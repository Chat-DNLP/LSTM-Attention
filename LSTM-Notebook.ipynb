{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2: Implementación de un mecanismo de atención en un modelo Seq2Seq con LSTMs\n",
    "\n",
    "Partiendo del código del modelo seq2seq con feedback para tareas de Traducción Automática Neuronal (NMT) del notebook anterior, se debe implementar el modelo de atención de Bahdanau o Luong.\n",
    "\n",
    "Objetivos de la práctica:\n",
    "- Entender el funcionamiento de los modelos Seq2Seq con LSTMs.\n",
    "- Comprender e implementar mecanismos de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from attention.attention_factory import AttentionFactory\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# !cd '/content/drive/My Drive/LSTM_attention' && ls\n",
    "\n",
    "# !pip uninstall torch torchtext -y\n",
    "# !pip install torch==2.0.1 torchtext==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install portalocker>=2.0.0\n",
    "\n",
    "# !python -m spacy download es_core_news_md\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dudas\n",
    "1. los métodos de atención se ajustan?\n",
    "2. Los lounge y badanau hay q hacerlos?\n",
    "3. Muestra de resultados: tets y train loss?\n",
    "4. Accuracy?\n",
    "5. RAM?\n",
    "6. Necesario un readme o memoria?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexión con *Weights & Biases*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msusanasrez\u001b[0m (\u001b[33mdata2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Susana\\Desktop\\Universidad\\Cuarto\\PLN\\Pract\\LSTM-Attention\\wandb\\run-20241030_123258-ldtoyav1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/data2023/LSTM-Attention/runs/ldtoyav1' target=\"_blank\">No-Attention</a></strong> to <a href='https://wandb.ai/data2023/LSTM-Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/data2023/LSTM-Attention' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/data2023/LSTM-Attention/runs/ldtoyav1' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention/runs/ldtoyav1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"LSTM-Attention\", name=\"Dot-product\",\n",
    "            config={\n",
    "          \"learning_rate\": 0.001,\n",
    "          \"architecture\": \"LSTM\",\n",
    "          \"epochs\": 30,\n",
    "          \"batch_size\": 7,\n",
    "          })\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translation(Dataset):\n",
    "    def __init__(self, source_file, target_file, train_size=0.9):\n",
    "        self.ingles = []\n",
    "        self.espanol = []\n",
    "        self.tokenizer_es = get_tokenizer(\"spacy\", language=\"es_core_news_md\")\n",
    "        self.tokenizer_en = get_tokenizer(\"spacy\", language=\"en_core_web_md\")\n",
    "        self.vocab_es = torchtext.vocab.FastText(language='es', unk_init=torch.Tensor.normal_)\n",
    "        self.vocab_en = torchtext.vocab.FastText(language='en', unk_init=torch.Tensor.normal_)\n",
    "\n",
    "        self.vocab_en = self.add_sos_eos_unk_pad(self.vocab_en)\n",
    "        self.vocab_es = self.add_sos_eos_unk_pad(self.vocab_es)\n",
    "\n",
    "        self.archivo_ingles = source_file\n",
    "        self.archivo_espanol = target_file\n",
    "\n",
    "        # Leer el conjunto de datos\n",
    "        for ingles, espanol in self.read_translation():\n",
    "            self.ingles.append(ingles)\n",
    "            self.espanol.append(espanol)\n",
    "        \n",
    "        # Dividir en entrenamiento y test\n",
    "        train_size = int(len(self) * train_size)\n",
    "        test_size = len(self) - train_size\n",
    "        self.train_dataset, self.test_dataset = random_split(self, [train_size, test_size])\n",
    "\n",
    "\n",
    "\n",
    "    def add_sos_eos_unk_pad(self, vocabulary):\n",
    "        words = vocabulary.itos\n",
    "        vocab = vocabulary.stoi\n",
    "        embedding_matrix = vocabulary.vectors\n",
    "\n",
    "        # Tokens especiales\n",
    "        sos_token = '<sos>'\n",
    "        eos_token = '<eos>'\n",
    "        pad_token = '<pad>'\n",
    "        unk_token = '<unk>'\n",
    "\n",
    "        # Inicializamos los vectores para los tokens especiales, por ejemplo, con ceros\n",
    "        sos_vector = torch.full((1, embedding_matrix.shape[1]), 1.)\n",
    "        eos_vector = torch.full((1, embedding_matrix.shape[1]), 2.)\n",
    "        pad_vector = torch.zeros((1, embedding_matrix.shape[1]))\n",
    "        unk_vector = torch.full((1, embedding_matrix.shape[1]), 3.)\n",
    "\n",
    "        # Añade los vectores al final de la matriz de embeddings\n",
    "        embedding_matrix = torch.cat((embedding_matrix, sos_vector, eos_vector, unk_vector, pad_vector), 0)\n",
    "\n",
    "        # Añade los tokens especiales al vocabulario\n",
    "        vocab[sos_token] = len(vocab)\n",
    "        vocab[eos_token] = len(vocab)\n",
    "        vocab[pad_token] = len(vocab)\n",
    "        vocab[unk_token] = len(vocab)\n",
    "\n",
    "        words.append(sos_token)\n",
    "        words.append(eos_token)\n",
    "        words.append(pad_token)\n",
    "        words.append(unk_token)\n",
    "\n",
    "        vocabulary.itos = words\n",
    "        vocabulary.stoi = vocab\n",
    "        vocabulary.vectors = embedding_matrix\n",
    "\n",
    "        default_stoi = defaultdict(lambda : len(vocabulary)-1, vocabulary.stoi)\n",
    "        vocabulary.stoi = default_stoi\n",
    "    \n",
    "        return vocabulary\n",
    "        \n",
    "\n",
    "    def read_translation(self):\n",
    "        with open(self.archivo_ingles, 'r', encoding='utf-8') as f_ingles, open(self.archivo_espanol, 'r', encoding='utf-8') as f_espanol:\n",
    "            for oracion_ingles, oracion_espanol in zip(f_ingles, f_espanol):\n",
    "                yield oracion_ingles.strip().lower(), oracion_espanol.strip().lower()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ingles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ingles[idx], self.espanol[idx]\n",
    "        tokens_ingles = self.tokenizer_en(item[0])\n",
    "        tokens_espanol = self.tokenizer_es(item[1])\n",
    "\n",
    "        tokens_ingles = tokens_ingles + ['<eos>']\n",
    "        tokens_espanol = ['<sos>'] + tokens_espanol + ['<eos>']\n",
    "\n",
    "        if not tokens_ingles or not tokens_espanol:\n",
    "            return torch.zeros(1, 300), torch.zeros(1, 300)\n",
    "            # raise RuntimeError(\"Una de las muestras está vacía.\")\n",
    "    \n",
    "        tensor_ingles = self.vocab_en.get_vecs_by_tokens(tokens_ingles)\n",
    "        tensor_espanol = self.vocab_es.get_vecs_by_tokens(tokens_espanol)\n",
    "\n",
    "        indices_ingles = [self.vocab_en.stoi[token] for token in tokens_ingles] + [self.vocab_en.stoi['<pad>']]\n",
    "        indices_espanol = [self.vocab_es.stoi[token] for token in tokens_espanol] + [self.vocab_es.stoi['<pad>']]\n",
    "\n",
    "        return tensor_ingles, tensor_espanol, indices_ingles, indices_espanol\n",
    "        \n",
    "            \n",
    "        \n",
    "def collate_fn(batch):\n",
    "    ingles_batch, espanol_batch, ingles_seqs, espanol_seqs = zip(*batch)\n",
    "    ingles_batch = pad_sequence(ingles_batch, batch_first=True, padding_value=0)\n",
    "    espanol_batch = pad_sequence(espanol_batch, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Calcular la longitud máxima de la lista de listas de índices\n",
    "    pad = espanol_seqs[0][-1]  # token <pad>\n",
    "    max_len = max([len(l) for l in espanol_seqs])\n",
    "    for seq in espanol_seqs:\n",
    "        seq += [pad]*(max_len-len(seq))\n",
    "        \n",
    "    return ingles_batch, espanol_batch, ingles_seqs, espanol_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_ingles = 'datasets_practice/mock/mock.en'\n",
    "archivo_espanol = 'datasets_practice/mock/mock.es'\n",
    "\n",
    "translation = Translation(archivo_ingles, archivo_espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definición del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__() \n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        return output, (hidden, cell)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, attention):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True) \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.attention = attention\n",
    "\n",
    "    # Modificar el forward para que haga la atención\n",
    "    def forward(self, x, hidden, cell, outputs_encoder):\n",
    "        output, (hidden, cell) = self.rnn(x, (hidden, cell))\n",
    "\n",
    "        # attention_weights -> [ batch_size = 8, 1 valor, X palabras en el encoder = 3] -> [ 8, 3]\n",
    "        attention_weights = self.attention.compute_score(output, outputs_encoder)\n",
    "\n",
    "        # Normalized vectors -> [ 8, 3, 1]\n",
    "        normalized_vectors = torch.softmax(attention_weights, dim=1).unsqueeze(-1)\n",
    "\n",
    "        # [ 8, 3, 512] * [ 8, 3, 512] = [8, 3, 512]\n",
    "        attention_output = normalized_vectors * outputs_encoder\n",
    "\n",
    "        # Promedio de los vectores -> [8, 1, 512]\n",
    "        summed_vectors = torch.sum(attention_output, dim=1, keepdim=True)\n",
    "\n",
    "        # output = [8,1,512]\n",
    "        output = self.fc_out(summed_vectors)\n",
    "        # output = [8,1, tamaño_vocab]\n",
    "        \n",
    "        return output, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder                           \n",
    "        self.es_embeddings = torchtext.vocab.FastText(language='es')\n",
    "        self.M = self.es_embeddings.vectors\n",
    "        self.M = torch.cat((self.M, torch.zeros((4, self.M.shape[1]))), 0)\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        target_len = target.shape[1]\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        # Tensor para almacenar las salidas del decoder\n",
    "        outputs = torch.zeros(batch_size, target_len, 985671)\n",
    "        \n",
    "        # Primero, la fuente es procesada por el encoder\n",
    "        outputs_encoder, (hidden, cell) = self.encoder(source)\n",
    "\n",
    "        # La primera entrada al decoder es el vector <sos>\n",
    "        x = target[:, 0, :]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, (hidden, cell) = self.decoder(x.unsqueeze(1), hidden, cell, outputs_encoder)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            if teacher_force:\n",
    "                x = target[:, t, :]\n",
    "            else:\n",
    "                x = torch.matmul(output.squeeze(1), self.M)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "attention = AttentionFactory.initialize_attention(\"Multi-Layer Perceptron\")\n",
    "\n",
    "# Inicializa el modelo, el optimizador y la función de pérdida\n",
    "encoder = Encoder(input_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(input_dim, hidden_dim, output_dim, num_layers, attention=attention)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#dataloader = DataLoader(translation, batch_size=config.batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "train_loader = DataLoader(translation.train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(translation.test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/2], Loss: 41.3928\n",
      "Epoch [1/30], Average Train Loss: 34.4032, Average Test Loss: 40.7221\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [2/30], Step [1/2], Loss: 40.6217\n",
      "Epoch [2/30], Average Train Loss: 40.2321, Average Test Loss: 38.4472\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [3/30], Step [1/2], Loss: 37.8623\n",
      "Epoch [3/30], Average Train Loss: 30.6154, Average Test Loss: 33.5300\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [4/30], Step [1/2], Loss: 32.3306\n",
      "Epoch [4/30], Average Train Loss: 31.6476, Average Test Loss: 29.7217\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [5/30], Step [1/2], Loss: 27.6367\n",
      "Epoch [5/30], Average Train Loss: 27.0376, Average Test Loss: 26.3548\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [6/30], Step [1/2], Loss: 23.8837\n",
      "Epoch [6/30], Average Train Loss: 18.2993, Average Test Loss: 24.0572\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [7/30], Step [1/2], Loss: 20.5263\n",
      "Epoch [7/30], Average Train Loss: 15.7869, Average Test Loss: 22.8080\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [8/30], Step [1/2], Loss: 17.5535\n",
      "Epoch [8/30], Average Train Loss: 18.0165, Average Test Loss: 22.5393\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [9/30], Step [1/2], Loss: 15.8266\n",
      "Epoch [9/30], Average Train Loss: 16.2906, Average Test Loss: 21.7554\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [10/30], Step [1/2], Loss: 14.5264\n",
      "Epoch [10/30], Average Train Loss: 11.2964, Average Test Loss: 20.8084\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [11/30], Step [1/2], Loss: 12.3795\n",
      "Epoch [11/30], Average Train Loss: 12.9400, Average Test Loss: 20.1160\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [12/30], Step [1/2], Loss: 10.8775\n",
      "Epoch [12/30], Average Train Loss: 8.6911, Average Test Loss: 19.5164\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [13/30], Step [1/2], Loss: 9.3570\n",
      "Epoch [13/30], Average Train Loss: 7.4160, Average Test Loss: 19.2555\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [14/30], Step [1/2], Loss: 7.8791\n",
      "Epoch [14/30], Average Train Loss: 6.5579, Average Test Loss: 19.1563\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [15/30], Step [1/2], Loss: 6.9425\n",
      "Epoch [15/30], Average Train Loss: 5.9390, Average Test Loss: 19.1815\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [16/30], Step [1/2], Loss: 6.3444\n",
      "Epoch [16/30], Average Train Loss: 4.7319, Average Test Loss: 19.3132\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [17/30], Step [1/2], Loss: 5.1235\n",
      "Epoch [17/30], Average Train Loss: 5.4584, Average Test Loss: 19.6305\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [18/30], Step [1/2], Loss: 5.4347\n",
      "Epoch [18/30], Average Train Loss: 4.4564, Average Test Loss: 19.9047\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [19/30], Step [1/2], Loss: 4.3762\n",
      "Epoch [19/30], Average Train Loss: 5.0763, Average Test Loss: 20.1222\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [20/30], Step [1/2], Loss: 4.0412\n",
      "Epoch [20/30], Average Train Loss: 4.6821, Average Test Loss: 20.1055\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [21/30], Step [1/2], Loss: 4.3223\n",
      "Epoch [21/30], Average Train Loss: 3.5524, Average Test Loss: 20.1583\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [22/30], Step [1/2], Loss: 4.0108\n",
      "Epoch [22/30], Average Train Loss: 4.0823, Average Test Loss: 20.3509\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [23/30], Step [1/2], Loss: 3.7001\n",
      "Epoch [23/30], Average Train Loss: 4.3824, Average Test Loss: 20.4474\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [24/30], Step [1/2], Loss: 4.0381\n",
      "Epoch [24/30], Average Train Loss: 4.0130, Average Test Loss: 20.5999\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [25/30], Step [1/2], Loss: 3.9124\n",
      "Epoch [25/30], Average Train Loss: 3.8509, Average Test Loss: 20.6744\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [26/30], Step [1/2], Loss: 4.0728\n",
      "Epoch [26/30], Average Train Loss: 3.7146, Average Test Loss: 20.7225\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [27/30], Step [1/2], Loss: 3.6054\n",
      "Epoch [27/30], Average Train Loss: 3.5377, Average Test Loss: 20.7494\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [28/30], Step [1/2], Loss: 3.5610\n",
      "Epoch [28/30], Average Train Loss: 3.5655, Average Test Loss: 20.8462\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [29/30], Step [1/2], Loss: 3.5085\n",
      "Epoch [29/30], Average Train Loss: 3.2330, Average Test Loss: 20.8641\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [30/30], Step [1/2], Loss: 3.4920\n",
      "Epoch [30/30], Average Train Loss: 3.4010, Average Test Loss: 20.8069\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{config.epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_indices, tgt_indices in test_loader:\n",
    "            output = model(src, tgt)\n",
    "\n",
    "            tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "            loss = 0\n",
    "            for t in range(1, tgt.shape[1]):\n",
    "                loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "\n",
    "    wandb.log({\"Train loss\": total_loss,\"Test_loss\": test_loss})\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{config.epochs}], Average Train Loss: {total_loss / len(train_loader):.4f}, Average Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "    print('--------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/dot_product.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_loss</td><td>█▇▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂</td></tr><tr><td>Train loss</td><td>▇█▆▆▆▄▃▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_loss</td><td>20.80693</td></tr><tr><td>Train loss</td><td>6.80205</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">No-Attention</strong> at: <a href='https://wandb.ai/data2023/LSTM-Attention/runs/ldtoyav1' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention/runs/ldtoyav1</a><br/> View project at: <a href='https://wandb.ai/data2023/LSTM-Attention' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241030_123258-ldtoyav1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./models/dot_product.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "sentence = \"tiger\"\n",
    "\n",
    "# Convertir a vectores\n",
    "tokens = translation.tokenizer_en(sentence)\n",
    "tokens = tokens + ['<eos>']\n",
    "text_tensor = translation.vocab_en.get_vecs_by_tokens(tokens)\n",
    "text_tensor = text_tensor.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_outputs, (hidden, cell) = model.encoder(text_tensor)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "input_token = torch.tensor(translation.vocab_es.stoi['<sos>']).unsqueeze(0)\n",
    "input_token = translation.vocab_es.vectors[input_token].unsqueeze(0)\n",
    "    \n",
    "\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        output, (hidden, cell) = model.decoder(input_token, hidden, cell) # teacher_forcing_ratio=0.0\n",
    "        \n",
    "    # Obtener el token con la probabilidad más alta\n",
    "    best_guess = output.argmax(2).squeeze(0)\n",
    "    outputs.append(best_guess.item())\n",
    "        \n",
    "    # Si el token es <eos>, terminar la traducción\n",
    "    if best_guess == translation.vocab_es.stoi['<eos>']:\n",
    "        break\n",
    "        \n",
    "    # Utilizar la palabra predicha como la siguiente entrada al decoder\n",
    "    input_token = translation.vocab_es.vectors[best_guess].unsqueeze(0)\n",
    "        \n",
    "# Convertir los índices de salida a palabras\n",
    "translated_sentence = [translation.vocab_es.itos[idx] for idx in outputs]\n",
    "    \n",
    "result = ' '.join(translated_sentence)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
