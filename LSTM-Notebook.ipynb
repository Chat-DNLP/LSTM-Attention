{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2: Implementación de un mecanismo de atención en un modelo Seq2Seq con LSTMs\n",
    "\n",
    "Partiendo del código del modelo seq2seq con feedback para tareas de Traducción Automática Neuronal (NMT) del notebook anterior, se debe implementar el modelo de atención de Bahdanau o Luong.\n",
    "\n",
    "Objetivos de la práctica:\n",
    "- Entender el funcionamiento de los modelos Seq2Seq con LSTMs.\n",
    "- Comprender e implementar mecanismos de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from attention.attention_factory import AttentionFactory\n",
    "from translation import Translation, collate_fn\n",
    "\n",
    "# Modelo Seq2Seq\n",
    "from models_definition.seq2seq.encoder import Encoder\n",
    "from models_definition.seq2seq.decoder import Decoder\n",
    "from models_definition.seq2seq.seq2seq import Seq2Seq\n",
    "\n",
    "# Modelo Bahdanau\n",
    "from models_definition.bahdanau.encoder import Encoder as EncoderBahdanau\n",
    "from models_definition.bahdanau.decoder import Decoder as DecoderBahdanau\n",
    "from models_definition.bahdanau.bahdanau import Bahdanau\n",
    "\n",
    "# Modelo Luong\n",
    "from models_definition.luong.encoder import Encoder as EncoderLuong\n",
    "from models_definition.luong.decoder import Decoder as DecoderLuong\n",
    "from models_definition.luong.luong import Luong\n",
    "\n",
    "#import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- Modelo Loung \n",
    "- Modelo Badanauh:\n",
    "    - son lstm bidireccionales con la segunda entrada en reverse : OJO\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexión con *Weights & Biases*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msusanasrez\u001b[0m (\u001b[33mdata2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Susana\\Desktop\\Universidad\\Cuarto\\PLN\\Pract\\LSTM-Attention\\wandb\\run-20241105_102759-zqt0ffoe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/data2023/LSTM-Attention/runs/zqt0ffoe' target=\"_blank\">Bahdanau</a></strong> to <a href='https://wandb.ai/data2023/LSTM-Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/data2023/LSTM-Attention' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/data2023/LSTM-Attention/runs/zqt0ffoe' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention/runs/zqt0ffoe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"LSTM-Attention\", name=\"Bahdanau\",\n",
    "            config={\n",
    "          \"learning_rate\": 0.001,\n",
    "          \"architecture\": \"LSTM\",\n",
    "          \"epochs\": 30,\n",
    "          \"batch_size\": 7,\n",
    "          })\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instalar los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_ingles = 'datasets_practice/mock/mock.en'\n",
    "archivo_espanol = 'datasets_practice/mock/mock.es'\n",
    "\n",
    "translation = Translation(archivo_ingles, archivo_espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Entrenamiento Seq2Seq con atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "attention = AttentionFactory.initialize_attention(\"Multi-Layer Perceptron\", hidden_dim, hidden_dim)\n",
    "\n",
    "# Inicializa el modelo, el optimizador y la función de pérdida\n",
    "encoder = Encoder(input_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(input_dim, hidden_dim, output_dim, num_layers, attention=attention)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(translation.train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(translation.test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/2], Loss: 41.3289\n",
      "Epoch [1/30], Average Train Loss: 34.4250, Average Test Loss: 26.7123\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [2/30], Step [1/2], Loss: 40.3296\n",
      "Epoch [2/30], Average Train Loss: 32.5693, Average Test Loss: 22.1751\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [3/30], Step [1/2], Loss: 34.9326\n",
      "Epoch [3/30], Average Train Loss: 33.4764, Average Test Loss: 15.7554\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [4/30], Step [1/2], Loss: 24.3726\n",
      "Epoch [4/30], Average Train Loss: 24.7107, Average Test Loss: 15.6595\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [5/30], Step [1/2], Loss: 21.8719\n",
      "Epoch [5/30], Average Train Loss: 20.4075, Average Test Loss: 13.3263\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [6/30], Step [1/2], Loss: 14.7328\n",
      "Epoch [6/30], Average Train Loss: 12.1259, Average Test Loss: 15.8580\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [7/30], Step [1/2], Loss: 13.7477\n",
      "Epoch [7/30], Average Train Loss: 12.4548, Average Test Loss: 14.1961\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [8/30], Step [1/2], Loss: 9.2637\n",
      "Epoch [8/30], Average Train Loss: 9.1647, Average Test Loss: 13.6774\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [9/30], Step [1/2], Loss: 8.5047\n",
      "Epoch [9/30], Average Train Loss: 7.3746, Average Test Loss: 11.6289\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [10/30], Step [1/2], Loss: 6.8315\n",
      "Epoch [10/30], Average Train Loss: 5.1276, Average Test Loss: 11.6090\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [11/30], Step [1/2], Loss: 7.9022\n",
      "Epoch [11/30], Average Train Loss: 7.7217, Average Test Loss: 11.6865\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [12/30], Step [1/2], Loss: 7.6112\n",
      "Epoch [12/30], Average Train Loss: 7.2440, Average Test Loss: 12.0283\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [13/30], Step [1/2], Loss: 6.5169\n",
      "Epoch [13/30], Average Train Loss: 4.9490, Average Test Loss: 13.5370\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [14/30], Step [1/2], Loss: 5.2310\n",
      "Epoch [14/30], Average Train Loss: 5.5329, Average Test Loss: 13.2338\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [15/30], Step [1/2], Loss: 5.4362\n",
      "Epoch [15/30], Average Train Loss: 4.4092, Average Test Loss: 16.8316\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [16/30], Step [1/2], Loss: 4.9480\n",
      "Epoch [16/30], Average Train Loss: 4.3149, Average Test Loss: 15.9555\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [17/30], Step [1/2], Loss: 5.2116\n",
      "Epoch [17/30], Average Train Loss: 6.0163, Average Test Loss: 16.7976\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [18/30], Step [1/2], Loss: 3.8285\n",
      "Epoch [18/30], Average Train Loss: 5.1207, Average Test Loss: 17.2104\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [19/30], Step [1/2], Loss: 4.7003\n",
      "Epoch [19/30], Average Train Loss: 5.3167, Average Test Loss: 17.1614\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [20/30], Step [1/2], Loss: 4.9762\n",
      "Epoch [20/30], Average Train Loss: 4.1544, Average Test Loss: 17.1543\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [21/30], Step [1/2], Loss: 5.3536\n",
      "Epoch [21/30], Average Train Loss: 4.4554, Average Test Loss: 16.7275\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [22/30], Step [1/2], Loss: 4.0091\n",
      "Epoch [22/30], Average Train Loss: 3.4553, Average Test Loss: 16.7769\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [23/30], Step [1/2], Loss: 3.3460\n",
      "Epoch [23/30], Average Train Loss: 4.4436, Average Test Loss: 17.0406\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [24/30], Step [1/2], Loss: 3.5980\n",
      "Epoch [24/30], Average Train Loss: 3.7052, Average Test Loss: 17.9839\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [25/30], Step [1/2], Loss: 3.8230\n",
      "Epoch [25/30], Average Train Loss: 4.3155, Average Test Loss: 18.7796\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [26/30], Step [1/2], Loss: 2.5811\n",
      "Epoch [26/30], Average Train Loss: 3.9212, Average Test Loss: 19.9604\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [27/30], Step [1/2], Loss: 3.1356\n",
      "Epoch [27/30], Average Train Loss: 4.4391, Average Test Loss: 19.4205\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [28/30], Step [1/2], Loss: 3.0196\n",
      "Epoch [28/30], Average Train Loss: 3.0419, Average Test Loss: 18.4473\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [29/30], Step [1/2], Loss: 3.2968\n",
      "Epoch [29/30], Average Train Loss: 2.9925, Average Test Loss: 17.9394\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [30/30], Step [1/2], Loss: 2.5248\n",
      "Epoch [30/30], Average Train Loss: 3.0294, Average Test Loss: 18.0932\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{config.epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_indices, tgt_indices in test_loader:\n",
    "            output = model(src, tgt)\n",
    "\n",
    "            tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "            loss = 0\n",
    "            for t in range(1, tgt.shape[1]):\n",
    "                loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "\n",
    "    wandb.log({\"Train loss\": total_loss,\"Test_loss\": test_loss})\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{config.epochs}], Average Train Loss: {total_loss / len(train_loader):.4f}, Average Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "    print('--------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './training_models/no_attention_Seq2Seq.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Entrenamiento Bahdanau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "# Inicializa el modelo, el optimizador y la función de pérdida\n",
    "encoder = EncoderBahdanau(input_dim, hidden_dim, num_layers)\n",
    "decoder = DecoderBahdanau(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model = Bahdanau(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(translation.train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(translation.test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/2], Loss: 41.4318\n",
      "Epoch [1/30], Average Train Loss: 40.5970, Average Test Loss: 23.3346\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [2/30], Step [1/2], Loss: 35.0180\n",
      "Epoch [2/30], Average Train Loss: 27.0646, Average Test Loss: 13.1916\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [3/30], Step [1/2], Loss: 23.0014\n",
      "Epoch [3/30], Average Train Loss: 20.0131, Average Test Loss: 13.3988\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [4/30], Step [1/2], Loss: 16.7200\n",
      "Epoch [4/30], Average Train Loss: 14.3433, Average Test Loss: 9.8559\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [5/30], Step [1/2], Loss: 10.5424\n",
      "Epoch [5/30], Average Train Loss: 8.9326, Average Test Loss: 8.1712\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [6/30], Step [1/2], Loss: 11.7293\n",
      "Epoch [6/30], Average Train Loss: 8.7871, Average Test Loss: 4.3068\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [7/30], Step [1/2], Loss: 5.8430\n",
      "Epoch [7/30], Average Train Loss: 6.4222, Average Test Loss: 3.9266\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [8/30], Step [1/2], Loss: 6.5543\n",
      "Epoch [8/30], Average Train Loss: 7.5485, Average Test Loss: 3.6037\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [9/30], Step [1/2], Loss: 4.1252\n",
      "Epoch [9/30], Average Train Loss: 7.7736, Average Test Loss: 3.0368\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [10/30], Step [1/2], Loss: 5.8536\n",
      "Epoch [10/30], Average Train Loss: 6.5386, Average Test Loss: 4.1320\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [11/30], Step [1/2], Loss: 7.4065\n",
      "Epoch [11/30], Average Train Loss: 6.0653, Average Test Loss: 6.2840\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [12/30], Step [1/2], Loss: 7.3849\n",
      "Epoch [12/30], Average Train Loss: 6.3312, Average Test Loss: 10.0619\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [13/30], Step [1/2], Loss: 5.5570\n",
      "Epoch [13/30], Average Train Loss: 5.4766, Average Test Loss: 10.3106\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [14/30], Step [1/2], Loss: 8.3515\n",
      "Epoch [14/30], Average Train Loss: 7.5315, Average Test Loss: 10.6480\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [15/30], Step [1/2], Loss: 7.3836\n",
      "Epoch [15/30], Average Train Loss: 5.0077, Average Test Loss: 10.2748\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [16/30], Step [1/2], Loss: 6.9233\n",
      "Epoch [16/30], Average Train Loss: 7.7948, Average Test Loss: 9.3408\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [17/30], Step [1/2], Loss: 3.6285\n",
      "Epoch [17/30], Average Train Loss: 6.5494, Average Test Loss: 8.2698\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [18/30], Step [1/2], Loss: 7.2364\n",
      "Epoch [18/30], Average Train Loss: 5.0618, Average Test Loss: 8.3071\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [19/30], Step [1/2], Loss: 3.1785\n",
      "Epoch [19/30], Average Train Loss: 3.9164, Average Test Loss: 9.3228\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [20/30], Step [1/2], Loss: 3.6767\n",
      "Epoch [20/30], Average Train Loss: 3.3086, Average Test Loss: 8.5651\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [21/30], Step [1/2], Loss: 4.7976\n",
      "Epoch [21/30], Average Train Loss: 3.6751, Average Test Loss: 9.3408\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [22/30], Step [1/2], Loss: 5.2224\n",
      "Epoch [22/30], Average Train Loss: 5.7932, Average Test Loss: 10.0420\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [23/30], Step [1/2], Loss: 3.5150\n",
      "Epoch [23/30], Average Train Loss: 3.0057, Average Test Loss: 9.2415\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [24/30], Step [1/2], Loss: 2.7505\n",
      "Epoch [24/30], Average Train Loss: 2.2120, Average Test Loss: 10.3201\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [25/30], Step [1/2], Loss: 2.9295\n",
      "Epoch [25/30], Average Train Loss: 2.7623, Average Test Loss: 10.3259\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [26/30], Step [1/2], Loss: 3.4606\n",
      "Epoch [26/30], Average Train Loss: 3.9302, Average Test Loss: 9.0635\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [27/30], Step [1/2], Loss: 4.6278\n",
      "Epoch [27/30], Average Train Loss: 3.5327, Average Test Loss: 8.2924\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [28/30], Step [1/2], Loss: 3.7364\n",
      "Epoch [28/30], Average Train Loss: 2.9764, Average Test Loss: 7.6757\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [29/30], Step [1/2], Loss: 1.5891\n",
      "Epoch [29/30], Average Train Loss: 2.6282, Average Test Loss: 7.6258\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [30/30], Step [1/2], Loss: 2.1789\n",
      "Epoch [30/30], Average Train Loss: 1.9624, Average Test Loss: 7.3172\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{config.epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_indices, tgt_indices in test_loader:\n",
    "            output = model(src, tgt)\n",
    "\n",
    "            tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "            loss = 0\n",
    "            for t in range(1, tgt.shape[1]):\n",
    "                loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "\n",
    "    wandb.log({\"Train loss\": total_loss,\"Test_loss\": test_loss})\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{config.epochs}], Average Train Loss: {total_loss / len(train_loader):.4f}, Average Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "    print('--------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './training_models/bahdanau.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Luong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "lr = 0.001\n",
    "batch_size = 7\n",
    "\n",
    "# Inicializa el modelo, el optimizador y la función de pérdida\n",
    "encoder = EncoderLuong(input_dim, hidden_dim, num_layers)\n",
    "decoder = DecoderLuong(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model = Luong(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(translation.train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(translation.test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/2], Loss: 41.4839\n",
      "Epoch [1/30], Average Train Loss: 34.2284, Average Test Loss: 39.8299\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [2/30], Step [1/2], Loss: 39.2797\n",
      "Epoch [2/30], Average Train Loss: 31.7624, Average Test Loss: 36.1807\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [3/30], Step [1/2], Loss: 33.8748\n",
      "Epoch [3/30], Average Train Loss: 26.7794, Average Test Loss: 31.7229\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [4/30], Step [1/2], Loss: 18.6728\n",
      "Epoch [4/30], Average Train Loss: 23.4287, Average Test Loss: 29.2567\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [5/30], Step [1/2], Loss: 26.5393\n",
      "Epoch [5/30], Average Train Loss: 25.4322, Average Test Loss: 27.5471\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [6/30], Step [1/2], Loss: 23.5562\n",
      "Epoch [6/30], Average Train Loss: 18.0261, Average Test Loss: 26.3425\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [7/30], Step [1/2], Loss: 21.2869\n",
      "Epoch [7/30], Average Train Loss: 16.3410, Average Test Loss: 25.6879\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [8/30], Step [1/2], Loss: 20.0124\n",
      "Epoch [8/30], Average Train Loss: 14.6752, Average Test Loss: 25.0838\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [9/30], Step [1/2], Loss: 18.4683\n",
      "Epoch [9/30], Average Train Loss: 14.1094, Average Test Loss: 24.5042\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [10/30], Step [1/2], Loss: 17.1039\n",
      "Epoch [10/30], Average Train Loss: 13.0105, Average Test Loss: 23.8192\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [11/30], Step [1/2], Loss: 15.7214\n",
      "Epoch [11/30], Average Train Loss: 11.9024, Average Test Loss: 23.1123\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [12/30], Step [1/2], Loss: 14.3708\n",
      "Epoch [12/30], Average Train Loss: 11.0992, Average Test Loss: 22.4788\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [13/30], Step [1/2], Loss: 13.0374\n",
      "Epoch [13/30], Average Train Loss: 9.5561, Average Test Loss: 21.5644\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [14/30], Step [1/2], Loss: 11.7827\n",
      "Epoch [14/30], Average Train Loss: 8.3890, Average Test Loss: 20.8521\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [15/30], Step [1/2], Loss: 10.1029\n",
      "Epoch [15/30], Average Train Loss: 7.7829, Average Test Loss: 20.3032\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [16/30], Step [1/2], Loss: 8.9581\n",
      "Epoch [16/30], Average Train Loss: 6.5921, Average Test Loss: 19.7387\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [17/30], Step [1/2], Loss: 7.6593\n",
      "Epoch [17/30], Average Train Loss: 6.1188, Average Test Loss: 19.4126\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [18/30], Step [1/2], Loss: 6.7468\n",
      "Epoch [18/30], Average Train Loss: 5.6084, Average Test Loss: 28.6284\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [19/30], Step [1/2], Loss: 5.5239\n",
      "Epoch [19/30], Average Train Loss: 6.0503, Average Test Loss: 19.4225\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [20/30], Step [1/2], Loss: 5.2598\n",
      "Epoch [20/30], Average Train Loss: 4.6600, Average Test Loss: 19.4205\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [21/30], Step [1/2], Loss: 5.9796\n",
      "Epoch [21/30], Average Train Loss: 5.0234, Average Test Loss: 19.5641\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [22/30], Step [1/2], Loss: 4.5456\n",
      "Epoch [22/30], Average Train Loss: 12.7124, Average Test Loss: 26.3687\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [23/30], Step [1/2], Loss: 5.1241\n",
      "Epoch [23/30], Average Train Loss: 5.7282, Average Test Loss: 20.2561\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [24/30], Step [1/2], Loss: 5.1771\n",
      "Epoch [24/30], Average Train Loss: 4.3488, Average Test Loss: 20.2937\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [25/30], Step [1/2], Loss: 5.3519\n",
      "Epoch [25/30], Average Train Loss: 4.2230, Average Test Loss: 20.4520\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [26/30], Step [1/2], Loss: 5.0938\n",
      "Epoch [26/30], Average Train Loss: 5.1751, Average Test Loss: 20.4860\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [27/30], Step [1/2], Loss: 5.2280\n",
      "Epoch [27/30], Average Train Loss: 4.1865, Average Test Loss: 20.6608\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [28/30], Step [1/2], Loss: 4.8792\n",
      "Epoch [28/30], Average Train Loss: 5.0950, Average Test Loss: 20.7420\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [29/30], Step [1/2], Loss: 4.6077\n",
      "Epoch [29/30], Average Train Loss: 5.0762, Average Test Loss: 20.7431\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [30/30], Step [1/2], Loss: 4.4744\n",
      "Epoch [30/30], Average Train Loss: 3.7450, Average Test Loss: 20.6991\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "epochs=30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_indices, tgt_indices in test_loader:\n",
    "            output = model(src, tgt)\n",
    "\n",
    "            tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "            loss = 0\n",
    "            for t in range(1, tgt.shape[1]):\n",
    "                loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "\n",
    "    #wandb.log({\"Train loss\": total_loss,\"Test_loss\": test_loss})\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Average Train Loss: {total_loss / len(train_loader):.4f}, Average Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "    print('--------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crear carpeta, ejecutar las dos celdas seguidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './training_models/luong.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Luong(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): LSTM(300, 512, batch_first=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn): LSTM(300, 512, batch_first=True)\n",
       "    (linear): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=985671, bias=True)\n",
       "    (attention): BilinearAttention(\n",
       "      (W): Linear(in_features=512, out_features=512, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "encoder = EncoderLuong(input_dim, hidden_dim, num_layers)\n",
    "decoder = DecoderLuong(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model = Luong(encoder, decoder)\n",
    "\n",
    "model.load_state_dict(torch.load('./training_models/luong.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mi <eos>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"head\"\n",
    "\n",
    "# Convertir a vectores\n",
    "tokens = translation.tokenizer_en(sentence)\n",
    "tokens = tokens + ['<eos>']\n",
    "text_tensor = translation.vocab_en.get_vecs_by_tokens(tokens)\n",
    "text_tensor = text_tensor.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_outputs, (hidden, cell) = model.encoder(text_tensor)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "input_token = torch.tensor(translation.vocab_es.stoi['<sos>']).unsqueeze(0)\n",
    "input_token = translation.vocab_es.vectors[input_token].unsqueeze(0)\n",
    "    \n",
    "\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        output, (hidden, cell) = model.decoder(input_token, hidden, cell, encoder_outputs) # teacher_forcing_ratio=0.0\n",
    "        \n",
    "    # Obtener el token con la probabilidad más alta\n",
    "    best_guess = output.argmax(2).squeeze(0)\n",
    "    outputs.append(best_guess.item())\n",
    "        \n",
    "    # Si el token es <eos>, terminar la traducción\n",
    "    if best_guess == translation.vocab_es.stoi['<eos>']:\n",
    "        break\n",
    "        \n",
    "    # Utilizar la palabra predicha como la siguiente entrada al decoder\n",
    "    input_token = translation.vocab_es.vectors[best_guess].unsqueeze(0)\n",
    "        \n",
    "# Convertir los índices de salida a palabras\n",
    "translated_sentence = [translation.vocab_es.itos[idx] for idx in outputs]\n",
    "    \n",
    "result = ' '.join(translated_sentence)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
