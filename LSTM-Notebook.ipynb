{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2: Implementación de un mecanismo de atención en un modelo Seq2Seq con LSTMs\n",
    "\n",
    "Partiendo del código del modelo seq2seq con feedback para tareas de Traducción Automática Neuronal (NMT) del notebook anterior, se debe implementar el modelo de atención de Bahdanau o Luong.\n",
    "\n",
    "Objetivos de la práctica:\n",
    "- Entender el funcionamiento de los modelos Seq2Seq con LSTMs.\n",
    "- Comprender e implementar mecanismos de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from attention.attention_factory import AttentionFactory\n",
    "from translation import Translation, collate_fn\n",
    "\n",
    "# Modelo Seq2Seq\n",
    "from models_definition.seq2seq.encoder import Encoder\n",
    "from models_definition.seq2seq.decoder import Decoder\n",
    "from models_definition.seq2seq.seq2seq import Seq2Seq\n",
    "\n",
    "# Modelo Bahdanau\n",
    "from models_definition.bahdanau.encoder import Encoder as EncoderBahdanau\n",
    "from models_definition.bahdanau.decoder import Decoder as DecoderBahdanau\n",
    "from models_definition.bahdanau.bahdanau import Bahdanau\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- Modelo Loung \n",
    "- Modelo Badanauh:\n",
    "    - son lstm bidireccionales con la segunda entrada en reverse : OJO\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexión con *Weights & Biases*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msusanasrez\u001b[0m (\u001b[33mdata2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Susana\\Desktop\\Universidad\\Cuarto\\PLN\\Pract\\LSTM-Attention\\wandb\\run-20241105_102759-zqt0ffoe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/data2023/LSTM-Attention/runs/zqt0ffoe' target=\"_blank\">Bahdanau</a></strong> to <a href='https://wandb.ai/data2023/LSTM-Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/data2023/LSTM-Attention' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/data2023/LSTM-Attention/runs/zqt0ffoe' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention/runs/zqt0ffoe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"LSTM-Attention\", name=\"Bahdanau\",\n",
    "            config={\n",
    "          \"learning_rate\": 0.001,\n",
    "          \"architecture\": \"LSTM\",\n",
    "          \"epochs\": 30,\n",
    "          \"batch_size\": 7,\n",
    "          })\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_ingles = 'datasets_practice/mock/mock.en'\n",
    "archivo_espanol = 'datasets_practice/mock/mock.es'\n",
    "\n",
    "translation = Translation(archivo_ingles, archivo_espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Entrenamiento Seq2Seq con atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Encoder(input_dim, hidden_dim, num_layers)\n\u001b[0;32m     13\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(input_dim, hidden_dim, output_dim, num_layers, attention\u001b[38;5;241m=\u001b[39mattention)\n\u001b[1;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2Seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate)\n\u001b[0;32m     16\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[1;32mc:\\Users\\Susana\\Desktop\\Universidad\\Cuarto\\PLN\\Pract\\LSTM-Attention\\models_definition\\seq2seq\\seq2seq.py:11\u001b[0m, in \u001b[0;36mSeq2Seq.__init__\u001b[1;34m(self, encoder, decoder)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m encoder\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m decoder                           \n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mes_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFastText\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mes_embeddings\u001b[38;5;241m.\u001b[39mvectors\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM, torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mM\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\pln\\lib\\site-packages\\torchtext\\vocab\\vectors.py:230\u001b[0m, in \u001b[0;36mFastText.__init__\u001b[1;34m(self, language, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl_base\u001b[38;5;241m.\u001b[39mformat(language)\n\u001b[0;32m    229\u001b[0m name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(url)\n\u001b[1;32m--> 230\u001b[0m \u001b[38;5;28msuper\u001b[39m(FastText, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\pln\\lib\\site-packages\\torchtext\\vocab\\vectors.py:59\u001b[0m, in \u001b[0;36mVectors.__init__\u001b[1;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munk_init \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39mzero_ \u001b[38;5;28;01mif\u001b[39;00m unk_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m unk_init\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_vectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_vectors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\pln\\lib\\site-packages\\torchtext\\vocab\\vectors.py:170\u001b[0m, in \u001b[0;36mVectors.cache\u001b[1;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading vectors from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path_pt))\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitos, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstoi, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectors, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_pt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\pln\\lib\\site-packages\\torch\\serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\pln\\lib\\site-packages\\torch\\serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\pln\\lib\\site-packages\\torch\\serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1141\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1142\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\Susana\\anaconda3\\envs\\pln\\lib\\site-packages\\torch\\serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[0;32m   1110\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1112\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m   1116\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1117\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1118\u001b[0m         _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "attention = AttentionFactory.initialize_attention(\"Multi-Layer Perceptron\", hidden_dim, hidden_dim)\n",
    "\n",
    "# Inicializa el modelo, el optimizador y la función de pérdida\n",
    "encoder = Encoder(input_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(input_dim, hidden_dim, output_dim, num_layers, attention=attention)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(translation.train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(translation.test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/2], Loss: 41.3289\n",
      "Epoch [1/30], Average Train Loss: 34.4250, Average Test Loss: 26.7123\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [2/30], Step [1/2], Loss: 40.3296\n",
      "Epoch [2/30], Average Train Loss: 32.5693, Average Test Loss: 22.1751\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [3/30], Step [1/2], Loss: 34.9326\n",
      "Epoch [3/30], Average Train Loss: 33.4764, Average Test Loss: 15.7554\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [4/30], Step [1/2], Loss: 24.3726\n",
      "Epoch [4/30], Average Train Loss: 24.7107, Average Test Loss: 15.6595\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [5/30], Step [1/2], Loss: 21.8719\n",
      "Epoch [5/30], Average Train Loss: 20.4075, Average Test Loss: 13.3263\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [6/30], Step [1/2], Loss: 14.7328\n",
      "Epoch [6/30], Average Train Loss: 12.1259, Average Test Loss: 15.8580\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [7/30], Step [1/2], Loss: 13.7477\n",
      "Epoch [7/30], Average Train Loss: 12.4548, Average Test Loss: 14.1961\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [8/30], Step [1/2], Loss: 9.2637\n",
      "Epoch [8/30], Average Train Loss: 9.1647, Average Test Loss: 13.6774\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [9/30], Step [1/2], Loss: 8.5047\n",
      "Epoch [9/30], Average Train Loss: 7.3746, Average Test Loss: 11.6289\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [10/30], Step [1/2], Loss: 6.8315\n",
      "Epoch [10/30], Average Train Loss: 5.1276, Average Test Loss: 11.6090\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [11/30], Step [1/2], Loss: 7.9022\n",
      "Epoch [11/30], Average Train Loss: 7.7217, Average Test Loss: 11.6865\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [12/30], Step [1/2], Loss: 7.6112\n",
      "Epoch [12/30], Average Train Loss: 7.2440, Average Test Loss: 12.0283\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [13/30], Step [1/2], Loss: 6.5169\n",
      "Epoch [13/30], Average Train Loss: 4.9490, Average Test Loss: 13.5370\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [14/30], Step [1/2], Loss: 5.2310\n",
      "Epoch [14/30], Average Train Loss: 5.5329, Average Test Loss: 13.2338\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [15/30], Step [1/2], Loss: 5.4362\n",
      "Epoch [15/30], Average Train Loss: 4.4092, Average Test Loss: 16.8316\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [16/30], Step [1/2], Loss: 4.9480\n",
      "Epoch [16/30], Average Train Loss: 4.3149, Average Test Loss: 15.9555\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [17/30], Step [1/2], Loss: 5.2116\n",
      "Epoch [17/30], Average Train Loss: 6.0163, Average Test Loss: 16.7976\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [18/30], Step [1/2], Loss: 3.8285\n",
      "Epoch [18/30], Average Train Loss: 5.1207, Average Test Loss: 17.2104\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [19/30], Step [1/2], Loss: 4.7003\n",
      "Epoch [19/30], Average Train Loss: 5.3167, Average Test Loss: 17.1614\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [20/30], Step [1/2], Loss: 4.9762\n",
      "Epoch [20/30], Average Train Loss: 4.1544, Average Test Loss: 17.1543\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [21/30], Step [1/2], Loss: 5.3536\n",
      "Epoch [21/30], Average Train Loss: 4.4554, Average Test Loss: 16.7275\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [22/30], Step [1/2], Loss: 4.0091\n",
      "Epoch [22/30], Average Train Loss: 3.4553, Average Test Loss: 16.7769\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [23/30], Step [1/2], Loss: 3.3460\n",
      "Epoch [23/30], Average Train Loss: 4.4436, Average Test Loss: 17.0406\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [24/30], Step [1/2], Loss: 3.5980\n",
      "Epoch [24/30], Average Train Loss: 3.7052, Average Test Loss: 17.9839\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [25/30], Step [1/2], Loss: 3.8230\n",
      "Epoch [25/30], Average Train Loss: 4.3155, Average Test Loss: 18.7796\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [26/30], Step [1/2], Loss: 2.5811\n",
      "Epoch [26/30], Average Train Loss: 3.9212, Average Test Loss: 19.9604\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [27/30], Step [1/2], Loss: 3.1356\n",
      "Epoch [27/30], Average Train Loss: 4.4391, Average Test Loss: 19.4205\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [28/30], Step [1/2], Loss: 3.0196\n",
      "Epoch [28/30], Average Train Loss: 3.0419, Average Test Loss: 18.4473\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [29/30], Step [1/2], Loss: 3.2968\n",
      "Epoch [29/30], Average Train Loss: 2.9925, Average Test Loss: 17.9394\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [30/30], Step [1/2], Loss: 2.5248\n",
      "Epoch [30/30], Average Train Loss: 3.0294, Average Test Loss: 18.0932\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{config.epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_indices, tgt_indices in test_loader:\n",
    "            output = model(src, tgt)\n",
    "\n",
    "            tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "            loss = 0\n",
    "            for t in range(1, tgt.shape[1]):\n",
    "                loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "\n",
    "    wandb.log({\"Train loss\": total_loss,\"Test_loss\": test_loss})\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{config.epochs}], Average Train Loss: {total_loss / len(train_loader):.4f}, Average Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "    print('--------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './training_models/no_attention_Seq2Seq.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Entrenamiento Bahdanau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "# Inicializa el modelo, el optimizador y la función de pérdida\n",
    "encoder = EncoderBahdanau(input_dim, hidden_dim, num_layers)\n",
    "decoder = DecoderBahdanau(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model = Bahdanau(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(translation.train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(translation.test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/2], Loss: 41.4318\n",
      "Epoch [1/30], Average Train Loss: 40.5970, Average Test Loss: 23.3346\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [2/30], Step [1/2], Loss: 35.0180\n",
      "Epoch [2/30], Average Train Loss: 27.0646, Average Test Loss: 13.1916\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [3/30], Step [1/2], Loss: 23.0014\n",
      "Epoch [3/30], Average Train Loss: 20.0131, Average Test Loss: 13.3988\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [4/30], Step [1/2], Loss: 16.7200\n",
      "Epoch [4/30], Average Train Loss: 14.3433, Average Test Loss: 9.8559\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [5/30], Step [1/2], Loss: 10.5424\n",
      "Epoch [5/30], Average Train Loss: 8.9326, Average Test Loss: 8.1712\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [6/30], Step [1/2], Loss: 11.7293\n",
      "Epoch [6/30], Average Train Loss: 8.7871, Average Test Loss: 4.3068\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [7/30], Step [1/2], Loss: 5.8430\n",
      "Epoch [7/30], Average Train Loss: 6.4222, Average Test Loss: 3.9266\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [8/30], Step [1/2], Loss: 6.5543\n",
      "Epoch [8/30], Average Train Loss: 7.5485, Average Test Loss: 3.6037\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [9/30], Step [1/2], Loss: 4.1252\n",
      "Epoch [9/30], Average Train Loss: 7.7736, Average Test Loss: 3.0368\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [10/30], Step [1/2], Loss: 5.8536\n",
      "Epoch [10/30], Average Train Loss: 6.5386, Average Test Loss: 4.1320\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [11/30], Step [1/2], Loss: 7.4065\n",
      "Epoch [11/30], Average Train Loss: 6.0653, Average Test Loss: 6.2840\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [12/30], Step [1/2], Loss: 7.3849\n",
      "Epoch [12/30], Average Train Loss: 6.3312, Average Test Loss: 10.0619\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [13/30], Step [1/2], Loss: 5.5570\n",
      "Epoch [13/30], Average Train Loss: 5.4766, Average Test Loss: 10.3106\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [14/30], Step [1/2], Loss: 8.3515\n",
      "Epoch [14/30], Average Train Loss: 7.5315, Average Test Loss: 10.6480\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [15/30], Step [1/2], Loss: 7.3836\n",
      "Epoch [15/30], Average Train Loss: 5.0077, Average Test Loss: 10.2748\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [16/30], Step [1/2], Loss: 6.9233\n",
      "Epoch [16/30], Average Train Loss: 7.7948, Average Test Loss: 9.3408\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [17/30], Step [1/2], Loss: 3.6285\n",
      "Epoch [17/30], Average Train Loss: 6.5494, Average Test Loss: 8.2698\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [18/30], Step [1/2], Loss: 7.2364\n",
      "Epoch [18/30], Average Train Loss: 5.0618, Average Test Loss: 8.3071\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [19/30], Step [1/2], Loss: 3.1785\n",
      "Epoch [19/30], Average Train Loss: 3.9164, Average Test Loss: 9.3228\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [20/30], Step [1/2], Loss: 3.6767\n",
      "Epoch [20/30], Average Train Loss: 3.3086, Average Test Loss: 8.5651\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [21/30], Step [1/2], Loss: 4.7976\n",
      "Epoch [21/30], Average Train Loss: 3.6751, Average Test Loss: 9.3408\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [22/30], Step [1/2], Loss: 5.2224\n",
      "Epoch [22/30], Average Train Loss: 5.7932, Average Test Loss: 10.0420\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [23/30], Step [1/2], Loss: 3.5150\n",
      "Epoch [23/30], Average Train Loss: 3.0057, Average Test Loss: 9.2415\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [24/30], Step [1/2], Loss: 2.7505\n",
      "Epoch [24/30], Average Train Loss: 2.2120, Average Test Loss: 10.3201\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [25/30], Step [1/2], Loss: 2.9295\n",
      "Epoch [25/30], Average Train Loss: 2.7623, Average Test Loss: 10.3259\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [26/30], Step [1/2], Loss: 3.4606\n",
      "Epoch [26/30], Average Train Loss: 3.9302, Average Test Loss: 9.0635\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [27/30], Step [1/2], Loss: 4.6278\n",
      "Epoch [27/30], Average Train Loss: 3.5327, Average Test Loss: 8.2924\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [28/30], Step [1/2], Loss: 3.7364\n",
      "Epoch [28/30], Average Train Loss: 2.9764, Average Test Loss: 7.6757\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [29/30], Step [1/2], Loss: 1.5891\n",
      "Epoch [29/30], Average Train Loss: 2.6282, Average Test Loss: 7.6258\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [30/30], Step [1/2], Loss: 2.1789\n",
      "Epoch [30/30], Average Train Loss: 1.9624, Average Test Loss: 7.3172\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{config.epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_indices, tgt_indices in test_loader:\n",
    "            output = model(src, tgt)\n",
    "\n",
    "            tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "            loss = 0\n",
    "            for t in range(1, tgt.shape[1]):\n",
    "                loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "\n",
    "    wandb.log({\"Train loss\": total_loss,\"Test_loss\": test_loss})\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{config.epochs}], Average Train Loss: {total_loss / len(train_loader):.4f}, Average Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "    print('--------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './training_models/bahdanau.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bahdanau(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): LSTM(300, 512, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn): LSTM(300, 1024, batch_first=True)\n",
       "    (linear): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (fc_out): Linear(in_features=1024, out_features=985671, bias=True)\n",
       "    (attention): MLPAttention(\n",
       "      (W1): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "      (W2): Linear(in_features=1024, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "encoder = EncoderBahdanau(input_dim, hidden_dim, num_layers)\n",
    "decoder = DecoderBahdanau(input_dim, hidden_dim, output_dim, num_layers)\n",
    "model = Bahdanau(encoder, decoder)\n",
    "\n",
    "model.load_state_dict(torch.load('./training_models/bahdanau.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tu gato <eos>\n"
     ]
    }
   ],
   "source": [
    "sentence = \"my cat\"\n",
    "\n",
    "# Convertir a vectores\n",
    "tokens = translation.tokenizer_en(sentence)\n",
    "tokens = tokens + ['<eos>']\n",
    "text_tensor = translation.vocab_en.get_vecs_by_tokens(tokens)\n",
    "text_tensor = text_tensor.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_outputs, (hidden, cell) = model.encoder(text_tensor)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "input_token = torch.tensor(translation.vocab_es.stoi['<sos>']).unsqueeze(0)\n",
    "input_token = translation.vocab_es.vectors[input_token].unsqueeze(0)\n",
    "    \n",
    "\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        output, (hidden, cell) = model.decoder(input_token, hidden, cell, encoder_outputs) # teacher_forcing_ratio=0.0\n",
    "        \n",
    "    # Obtener el token con la probabilidad más alta\n",
    "    best_guess = output.argmax(2).squeeze(0)\n",
    "    outputs.append(best_guess.item())\n",
    "        \n",
    "    # Si el token es <eos>, terminar la traducción\n",
    "    if best_guess == translation.vocab_es.stoi['<eos>']:\n",
    "        break\n",
    "        \n",
    "    # Utilizar la palabra predicha como la siguiente entrada al decoder\n",
    "    input_token = translation.vocab_es.vectors[best_guess].unsqueeze(0)\n",
    "        \n",
    "# Convertir los índices de salida a palabras\n",
    "translated_sentence = [translation.vocab_es.itos[idx] for idx in outputs]\n",
    "    \n",
    "result = ' '.join(translated_sentence)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
