{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2: Implementación de un mecanismo de atención en un modelo Seq2Seq con LSTMs\n",
    "\n",
    "Partiendo del código del modelo seq2seq con feedback para tareas de Traducción Automática Neuronal (NMT) del notebook anterior, se debe implementar el modelo de atención de Bahdanau o Luong.\n",
    "\n",
    "Objetivos de la práctica:\n",
    "- Entender el funcionamiento de los modelos Seq2Seq con LSTMs.\n",
    "- Comprender e implementar mecanismos de atención."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from attention.attention_factory import AttentionFactory\n",
    "from translation import Translation, collate_fn\n",
    "from seq2seq.encoder import Encoder\n",
    "from seq2seq.decoder import Decoder\n",
    "from seq2seq.seq2seq import Seq2Seq\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dudas\n",
    "entregables: BAG of Words, LSTM Glove, LSTM-attention --> con readmes\n",
    "\n",
    "\n",
    "### TODO:\n",
    "- Modelo Loung \n",
    "- Modelo Badanauh:\n",
    "    - son lstm bidireccionales con la segunda entrada en reverse : OJO\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexión con *Weights & Biases*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msusanasrez\u001b[0m (\u001b[33mdata2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Susana\\Desktop\\Universidad\\Cuarto\\PLN\\Pract\\LSTM-Attention\\wandb\\run-20241103_164920-l2bcbfxf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/data2023/LSTM-Attention/runs/l2bcbfxf' target=\"_blank\">Dot-product</a></strong> to <a href='https://wandb.ai/data2023/LSTM-Attention' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/data2023/LSTM-Attention' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/data2023/LSTM-Attention/runs/l2bcbfxf' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention/runs/l2bcbfxf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"LSTM-Attention\", name=\"Dot-product\",\n",
    "            config={\n",
    "          \"learning_rate\": 0.001,\n",
    "          \"architecture\": \"LSTM\",\n",
    "          \"epochs\": 30,\n",
    "          \"batch_size\": 7,\n",
    "          })\n",
    "\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_ingles = 'datasets_practice/mock/mock.en'\n",
    "archivo_espanol = 'datasets_practice/mock/mock.es'\n",
    "\n",
    "translation = Translation(archivo_ingles, archivo_espanol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "attention = AttentionFactory.initialize_attention(\"Multi-Layer Perceptron\", hidden_dim, hidden_dim)\n",
    "\n",
    "# Inicializa el modelo, el optimizador y la función de pérdida\n",
    "encoder = Encoder(input_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(input_dim, hidden_dim, output_dim, num_layers, attention=attention)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#dataloader = DataLoader(translation, batch_size=config.batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "train_loader = DataLoader(translation.train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(translation.test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Step [1/2], Loss: 41.3668\n",
      "Epoch [1/30], Average Train Loss: 40.9661, Average Test Loss: 36.8135\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [2/30], Step [1/2], Loss: 36.5829\n",
      "Epoch [2/30], Average Train Loss: 28.8277, Average Test Loss: 26.5496\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [3/30], Step [1/2], Loss: 16.2330\n",
      "Epoch [3/30], Average Train Loss: 21.3267, Average Test Loss: 22.8025\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [4/30], Step [1/2], Loss: 21.1832\n",
      "Epoch [4/30], Average Train Loss: 15.7309, Average Test Loss: 18.9899\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [5/30], Step [1/2], Loss: 14.4041\n",
      "Epoch [5/30], Average Train Loss: 11.5986, Average Test Loss: 15.1740\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [6/30], Step [1/2], Loss: 11.0621\n",
      "Epoch [6/30], Average Train Loss: 7.3851, Average Test Loss: 13.6560\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [7/30], Step [1/2], Loss: 9.4689\n",
      "Epoch [7/30], Average Train Loss: 7.7459, Average Test Loss: 13.0627\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [8/30], Step [1/2], Loss: 4.7936\n",
      "Epoch [8/30], Average Train Loss: 6.4493, Average Test Loss: 14.4907\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [9/30], Step [1/2], Loss: 4.4000\n",
      "Epoch [9/30], Average Train Loss: 5.4062, Average Test Loss: 13.7618\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [10/30], Step [1/2], Loss: 4.8299\n",
      "Epoch [10/30], Average Train Loss: 5.0458, Average Test Loss: 14.7327\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [11/30], Step [1/2], Loss: 5.8558\n",
      "Epoch [11/30], Average Train Loss: 5.8125, Average Test Loss: 14.4408\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [12/30], Step [1/2], Loss: 3.9004\n",
      "Epoch [12/30], Average Train Loss: 5.0259, Average Test Loss: 13.8883\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "Epoch [13/30], Step [1/2], Loss: 5.3895\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{config.epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt, src_indices, tgt_indices in test_loader:\n",
    "            output = model(src, tgt)\n",
    "\n",
    "            tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "            loss = 0\n",
    "            for t in range(1, tgt.shape[1]):\n",
    "                loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "\n",
    "    wandb.log({\"Train loss\": total_loss,\"Test_loss\": test_loss})\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{config.epochs}], Average Train Loss: {total_loss / len(train_loader):.4f}, Average Test Loss: {test_loss / len(test_loader):.4f}')\n",
    "    print('--------------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/dot_product.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test_loss</td><td>█▇▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂</td></tr><tr><td>Train loss</td><td>▇█▆▆▆▄▃▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test_loss</td><td>20.80693</td></tr><tr><td>Train loss</td><td>6.80205</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">No-Attention</strong> at: <a href='https://wandb.ai/data2023/LSTM-Attention/runs/ldtoyav1' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention/runs/ldtoyav1</a><br/> View project at: <a href='https://wandb.ai/data2023/LSTM-Attention' target=\"_blank\">https://wandb.ai/data2023/LSTM-Attention</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241030_123258-ldtoyav1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./models/dot_product.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "sentence = \"tiger\"\n",
    "\n",
    "# Convertir a vectores\n",
    "tokens = translation.tokenizer_en(sentence)\n",
    "tokens = tokens + ['<eos>']\n",
    "text_tensor = translation.vocab_en.get_vecs_by_tokens(tokens)\n",
    "text_tensor = text_tensor.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_outputs, (hidden, cell) = model.encoder(text_tensor)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "input_token = torch.tensor(translation.vocab_es.stoi['<sos>']).unsqueeze(0)\n",
    "input_token = translation.vocab_es.vectors[input_token].unsqueeze(0)\n",
    "    \n",
    "\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        output, (hidden, cell) = model.decoder(input_token, hidden, cell) # teacher_forcing_ratio=0.0\n",
    "        \n",
    "    # Obtener el token con la probabilidad más alta\n",
    "    best_guess = output.argmax(2).squeeze(0)\n",
    "    outputs.append(best_guess.item())\n",
    "        \n",
    "    # Si el token es <eos>, terminar la traducción\n",
    "    if best_guess == translation.vocab_es.stoi['<eos>']:\n",
    "        break\n",
    "        \n",
    "    # Utilizar la palabra predicha como la siguiente entrada al decoder\n",
    "    input_token = translation.vocab_es.vectors[best_guess].unsqueeze(0)\n",
    "        \n",
    "# Convertir los índices de salida a palabras\n",
    "translated_sentence = [translation.vocab_es.itos[idx] for idx in outputs]\n",
    "    \n",
    "result = ' '.join(translated_sentence)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
